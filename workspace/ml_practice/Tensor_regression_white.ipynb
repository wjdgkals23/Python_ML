{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "filename = \"data/white.csv\"\n",
    "\n",
    "# setup text reader\n",
    "file_length = file_len(filename)\n",
    "filename_queue = tf.train.string_input_producer([filename])\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "_, csv_row = reader.read(filename_queue)\n",
    "\n",
    "# setup CSV decoding\n",
    "record_defaults = [[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0]]\n",
    "x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,label= tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
    "label = tf.one_hot(label,depth=7)\n",
    "\n",
    "features = tf.stack([x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.000e+00 2.700e-01 3.600e-01 2.070e+01 4.500e-02 4.500e+01 1.700e+02\n",
      " 1.001e+00 3.000e+00 4.500e-01 8.800e+00]\n",
      "[6.30e+00 3.00e-01 3.40e-01 1.60e+00 4.90e-02 1.40e+01 1.32e+02 9.94e-01\n",
      " 3.30e+00 4.90e-01 9.50e+00]\n",
      "[8.100e+00 2.800e-01 4.000e-01 6.900e+00 5.000e-02 3.000e+01 9.700e+01\n",
      " 9.951e-01 3.260e+00 4.400e-01 1.010e+01]\n",
      "[7.200e+00 2.300e-01 3.200e-01 8.500e+00 5.800e-02 4.700e+01 1.860e+02\n",
      " 9.956e-01 3.190e+00 4.000e-01 9.900e+00]\n",
      "[7.200e+00 2.300e-01 3.200e-01 8.500e+00 5.800e-02 4.700e+01 1.860e+02\n",
      " 9.956e-01 3.190e+00 4.000e-01 9.900e+00]\n",
      "[8.100e+00 2.800e-01 4.000e-01 6.900e+00 5.000e-02 3.000e+01 9.700e+01\n",
      " 9.951e-01 3.260e+00 4.400e-01 1.010e+01]\n",
      "[6.200e+00 3.200e-01 1.600e-01 7.000e+00 4.500e-02 3.000e+01 1.360e+02\n",
      " 9.949e-01 3.180e+00 4.700e-01 9.600e+00]\n",
      "[7.000e+00 2.700e-01 3.600e-01 2.070e+01 4.500e-02 4.500e+01 1.700e+02\n",
      " 1.001e+00 3.000e+00 4.500e-01 8.800e+00]\n",
      "[6.30e+00 3.00e-01 3.40e-01 1.60e+00 4.90e-02 1.40e+01 1.32e+02 9.94e-01\n",
      " 3.30e+00 4.90e-01 9.50e+00]\n",
      "[8.100e+00 2.200e-01 4.300e-01 1.500e+00 4.400e-02 2.800e+01 1.290e+02\n",
      " 9.938e-01 3.220e+00 4.500e-01 1.100e+01]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "filename = \"data/white.csv\"\n",
    "\n",
    "# setup text reader\n",
    "file_length = file_len(filename)\n",
    "filename_queue = tf.train.string_input_producer([filename])\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "_, csv_row = reader.read(filename_queue, )\n",
    "\n",
    "# setup CSV decoding\n",
    "record_defaults = [[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0]]\n",
    "x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,label= tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
    "label = tf.one_hot(label,depth=7)\n",
    "\n",
    "\n",
    "features = tf.stack([x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11])\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 11]) # 학습데이터는 11개의 features\n",
    "Y = tf.placeholder(tf.float32, [None, 7]) # 라벨 데이터는 7가지의 label\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout rate\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[11, 44], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([44])) # bias는 가중치의 열의 개수에 맞춘다.\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[44,22], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([22])) # bias는 가중치의 열의 개수에 맞춘다.\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[22,7], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([7])) # bias는 가중치의 열의 개수에 맞춘다.\n",
    "L3 = tf.nn.relu(tf.matmul(L2,W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[7,2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([2])) # bias는 가중치의 열의 개수에 맞춘다.\n",
    "\n",
    "regression = tf.matmul(L2, W3) + b3 # 최종 그래프\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=regression, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam 짱....\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "      # start populating filename queue\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        for i in range(500):\n",
    "        # retrieve a single instance\n",
    "            for j in range(file_length):\n",
    "                example,labels = sess.run([features,label])\n",
    "                feed_dict = {X:example, Y:labels, keep_prob: 0.7}\n",
    "                c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "                avg = avg + c\n",
    "            avg = avg/file_length\n",
    "            print('cost =', '{:.4f}'.format(avg))\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "  # start populating filename queue\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(10):\n",
    "    # retrieve a single instance\n",
    "        example,labels = sess.run([features,label])\n",
    "        print(example)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 11]) # 학습데이터는 11개의 features\n",
    "Y = tf.placeholder(tf.float32, [None, 7]) # 라벨 데이터는 7가지의 label\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout rate\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[11, 44], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([44])) # bias는 가중치의 열의 개수에 맞춘다.\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[44,22], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([22])) # bias는 가중치의 열의 개수에 맞춘다.\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[22,7], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([7])) # bias는 가중치의 열의 개수에 맞춘다.\n",
    "\n",
    "regression = tf.matmul(L2, W3) + b3 # 최종 그래프\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=regression, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam 짱....\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "  # start populating filename queue\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    for i in range(500):\n",
    "    # retrieve a single instance\n",
    "        for j in range(file_length)\n",
    "            example,labels = sess.run([features,label])\n",
    "            feed_dict = {X:example, Y:labels, keep_prob: 0.7}\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "            avg = avg + c\n",
    "        avg = avg/file_length\n",
    "        print('cost =', '{:.4f}'.format(avg))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
