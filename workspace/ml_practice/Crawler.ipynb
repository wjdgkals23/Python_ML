{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 이런 스타일로 입고 싶은데 내가 가지고 있는걸로는 어떻게 입을 수 있을까?\n",
    "#### input: 원하는 스타일 및 가지고 있는 상품종류 -> ex) 스포티 / 카라티 \n",
    "#### output 이미지 / 추가 착용아이템 리스트  -> ex) 대표이미지 / 오픈넥셔츠 숏팬츠\n",
    "#### 무신사 대표적인 스타일을 보여주는 스탭 스냅 사이트를 크롤링한다.\n",
    "#### 1차 수집 : http://www.musinsa.com/index.php?m=street&_mon=&p=1#listStart에 들어있는 모든 url을 수집한다.\n",
    "#### 2차 수집 수집한 url에서 원하는 정보를 추출한다 태그 / 스타일 / 날짜 \n",
    "\n",
    "### 시간상 32000개의 데이터 중 2000개만 수집\n",
    "\n",
    "import sys\n",
    "import io\n",
    "# sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding = 'utf-8')\n",
    "# sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding = 'utf-8')\n",
    "import odbc\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "try:\n",
    "    import urllib.request as urllib2\n",
    "except ImportError:\n",
    "    import urllib2\n",
    "\n",
    "#### DB 연결하는 방식으로 했습니다,\n",
    "connect = odbc.odbc('first')\n",
    "db = connect.cursor()\n",
    "\n",
    "def login_musinsa(dri):\n",
    "    dri.implicitly_wait(3)   # 암묵적으로 웹 자원 로드를 위해 3초까지 기다려 준다.\n",
    "    url=\"https://www.musinsa.com/?mod=login&referer=http%3A%2F%2Fstore.musinsa.com%2Fapp%2F\"\n",
    "    dri.get(url)   # url에 접근한다.\n",
    "    dri.find_element_by_name(\"id\").send_keys(\"wjdgkals23\")\n",
    "    dri.find_element_by_name(\"pw\").send_keys(\"bn150525268b\")\n",
    "    dri.find_element_by_xpath(\"/html/body/div[1]/div[2]/form/span[3]/input\").send_keys(Keys.ENTER)\n",
    "\n",
    "def parse_url(dri,num):\n",
    "    link_list = []\n",
    "    for i in range(1,num):\n",
    "        s = str(i)\n",
    "        print(s)\n",
    "        link = \"http://www.musinsa.com/index.php?m=street&_mon=&p=\"+s+\"#listStart\"\n",
    "        req = urllib2.urlopen(link)\n",
    "        soup = BeautifulSoup(req, 'html.parser')\n",
    "        f_div = soup.find(\"div\",\"boxed-list-wrapper\")\n",
    "        f_ul = f_div.find(\"ul\",\"snap-article-list boxed-article-list article-list center list\")\n",
    "        f_li_list = f_ul.find_all(\"li\",\"listItem\")\n",
    "        for li in f_li_list:\n",
    "            s_div = li.find(\"div\",\"articleImg\")\n",
    "            href = s_div.find(\"a\")['href']\n",
    "            link_list.append(\"http://www.musinsa.com\" + href)\n",
    "    return link_list\n",
    "\n",
    "def crawl_each(dri, urls, db):\n",
    "    info_list = []\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "#         req = urllib2.urlopen(url)\n",
    "        dri.get(url)\n",
    "        soup = BeautifulSoup(dri.page_source, 'html.parser')\n",
    "        div = soup.find(\"div\", \"snapInfo\")\n",
    "        div_table = div.find(\"table\")\n",
    "        trs = div_table.find_all(\"tr\")\n",
    "        page_dict = {}\n",
    "        tag_sentence = \"\"\n",
    "        gender = \"\"\n",
    "        style = \"\"\n",
    "        for tr in trs:\n",
    "            th = tr.find(\"th\").text\n",
    "            th = th.strip()\n",
    "            if(th == \"태그\"):\n",
    "                td = tr.find(\"td\")\n",
    "                ul = td.find(\"ul\")\n",
    "                lis = ul.find_all(\"li\")\n",
    "                for li in lis:\n",
    "                    span = li.find(\"span\")\n",
    "                    if(span.text == \"남자\" or span.text == \"여자\"):\n",
    "                        gender = span.text\n",
    "                    else:\n",
    "                        tag_sentence = tag_sentence + span.text + \"/\"\n",
    "            elif(th == \"스타일\"):    \n",
    "                td = tr.find(\"td\")\n",
    "                span = td.find(\"span\")\n",
    "                style = span.text\n",
    "            else:\n",
    "                cnt = 1\n",
    "            if(style != \"\" and tag_sentence != \"\" and gender != \"\"):\n",
    "#                 page_dict['style'] = style\n",
    "#                 page_dict['tag'] = tag_sentence\n",
    "#                 page_dict['gender'] = gender\n",
    "#                 page_dict['url'] = url\n",
    "                db.execute(\"INSERT into first.musinsa_info (style, tags, gender, url) VALUES ('%s','%s','%s', '%s')\" % (style, tag_sentence, gender, url))\n",
    "                info_list.append(page_dict)\n",
    "\n",
    "driver = webdriver.Chrome(\"C:\\chromedriver\\chromedriver\")\n",
    "login_musinsa(driver)\n",
    "url_link = parse_url(driver, 30) # 30 페이지 url 수집 (30*60 = 1800개 정보 수집)\n",
    "info_list = crawl_each(driver, url_link, db)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
